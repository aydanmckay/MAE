{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b9e666-51a7-4d74-af3f-097b613100dd",
   "metadata": {},
   "source": [
    "## Training a Masked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8753c102-5299-492f-a620-ce3bbbb7d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "from time import time\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gaiaxpy import generate, PhotometricSystem\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.size'] = 5.0\n",
    "plt.rcParams['xtick.minor.size'] = 3.0\n",
    "plt.rcParams['ytick.major.size'] = 5.0\n",
    "plt.rcParams['ytick.minor.size'] = 3.0\n",
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['ytick.right'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b3b90-6b10-4371-915b-e53eea83e19e",
   "metadata": {},
   "source": [
    "Converting to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a2ad50-1678-475c-9fd9-6816409c005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfe9eb-9235-4bd0-808c-f4cf267d12d5",
   "metadata": {},
   "source": [
    "Checking directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0499d6-8684-459c-9d47-9e6ac3d8d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /scratch/\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8731b87-ad6e-4b5b-bdf0-6fdff9bda93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers for dataloading\n",
    "metscaler = StandardScaler(); logscaler = StandardScaler(); tefscaler = StandardScaler()\n",
    "# extscaler = StandardScaler(); parscaler = StandardScaler()\n",
    "scale = 'standard_scale'\n",
    "\n",
    "batchlen = 32\n",
    "lr = 1e-4\n",
    "epochs = 10\n",
    "optimize = 'Adam'\n",
    "datafname = \"/arc/home/aydanmckay/mae_tab/lamost_pristine_bprp_gmag.h5\"\n",
    "datashort = 'ViT_MAE_v1'\n",
    "lossname = 'L2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39544205-8c90-441f-a866-44750d5bb643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the Dataset class\n",
    "class data_set(Dataset):\n",
    "    '''\n",
    "    Main way to access the .h5 file.\n",
    "    '''\n",
    "    def __init__(self,file,train=True,valid=False,test=False,noscale=False):\n",
    "        fn = h5py.File(file, 'r')\n",
    "        self.f = fn\n",
    "        \n",
    "        # get data\n",
    "        if train:\n",
    "            name = 'group_1'\n",
    "        elif valid:\n",
    "            name = 'group_2'\n",
    "        elif test or noscale:\n",
    "            name = 'group_3'\n",
    "        \n",
    "        dset = self.f[name]['theta']\n",
    "        dl = dset[:]\n",
    "        if noscale:\n",
    "            self.l = dl.shape[1]\n",
    "            self.t = torch.Tensor(dl.T)\n",
    "        else:\n",
    "            dat = np.array([\n",
    "                metscaler.fit_transform(dl[[0]].T).flatten(),\n",
    "                logscaler.fit_transform(dl[[1]].T).flatten(),\n",
    "                tefscaler.fit_transform(dl[[2]].T).flatten(),\n",
    "            ])\n",
    "            self.l = dat.shape[1]\n",
    "            self.x = torch.Tensor(dat.T)\n",
    "\n",
    "        ydset = self.f[name]['bprp']\n",
    "        ydat = ydset[:]\n",
    "        self.y = torch.Tensor(ydat[:].T)\n",
    "\n",
    "        errdset = self.f[name]['e_bprp']\n",
    "        self.err = torch.Tensor(errdset[:].T)\n",
    "        \n",
    "        mdset = self.f[name]['mags']\n",
    "        self.m = torch.Tensor(mdset[:].T)\n",
    "        \n",
    "        ddset = self.f[name]['dist']\n",
    "        self.d = torch.Tensor(ddset[:].T)\n",
    "        \n",
    "        edset = self.f[name]['ext']\n",
    "        self.e = torch.Tensor(edset[:].T)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        tg = self.t[index]\n",
    "        yg = self.y[index]\n",
    "        mg = self.m[index]\n",
    "        errg = self.err[index]\n",
    "        eg = self.e[index]\n",
    "        dg = self.d[index]\n",
    "        return (tg,yg,errg,mg,eg,dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adea164b-8f9e-405e-bdd6-5503675f2af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatgpt TabularViT\n",
    "# this will be the encoder\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Define the TabularViT model\n",
    "class TabularViT(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, patch_dim=64, num_patches=16, dim=256, depth=6, heads=8, mlp_dim=512):\n",
    "        super().__init__()\n",
    "        self.patch_dim = patch_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.to_patch_embedding = nn.Linear(input_dim, patch_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, dim))\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim), num_layers=depth)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.fc = nn.Linear(dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x).transpose(1, 2)\n",
    "        x = self._add_positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.layer_norm(x.mean(dim=1))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _add_positional_encoding(self, x):\n",
    "        b, n, _ = x.shape\n",
    "        position_embeddings = self.position_embeddings[:, :(n + 1)]\n",
    "        return (x + position_embeddings).permute(1, 0, 2)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, accuracy\n",
    "\n",
    "# Define the validation loop\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434947fd-bf47-4e8f-b59e-68533cc230f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE from chatgpt\n",
    "# switch encoder for above tab_vit\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, mask):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        masked_x = x * self.mask\n",
    "        z = self.encoder(masked_x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4d5e1e-f866-4176-b58e-22a35e8fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_set(datafname)\n",
    "valid_data = data_set(datafname,train=False,valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba5b7ec-3c42-404a-ab87-43abea670c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batchlen,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=batchlen,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9be5412-c330-4ee2-b7f6-3d12fa466df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMAE\u001b[49m()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAE' is not defined"
     ]
    }
   ],
   "source": [
    "model = MAE()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc998bc-a8f0-41c5-bdce-b846e1032f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data.multi_view_collate import MultiViewCollate\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import masked_autoencoder\n",
    "from lightly.transforms.mae_transform import MAETransform\n",
    "\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_dim = 512\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_size\n",
    "        self.sequence_length = vit.seq_length\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        self.backbone = masked_autoencoder.MAEBackbone.from_vit(vit)\n",
    "        self.decoder = masked_autoencoder.MAEDecoder(\n",
    "            seq_length=vit.seq_length,\n",
    "            num_layers=1,\n",
    "            num_heads=16,\n",
    "            embed_input_dim=vit.hidden_dim,\n",
    "            hidden_dim=decoder_dim,\n",
    "            mlp_dim=decoder_dim * 4,\n",
    "            out_dim=vit.patch_size**2 * 3,\n",
    "            dropout=0,\n",
    "            attention_dropout=0,\n",
    "        )\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images, idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images, idx_keep)\n",
    "        x_pred = self.forward_decoder(x_encoded, idx_keep, idx_mask)\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "        return x_pred, target\n",
    "\n",
    "\n",
    "vit = torchvision.models.vit_b_32(pretrained=False)\n",
    "model = MAE(vit)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "pascal_voc = torchvision.datasets.VOCDetection(\n",
    "    \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    ")\n",
    "transform = MAETransform()\n",
    "dataset = LightlyDataset.from_torch_dataset(pascal_voc, transform=transform)\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")\n",
    "\n",
    "collate_fn = MultiViewCollate()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9d282-8884-4bec-83f6-3381ff42c630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, _, _ in dataloader:\n",
    "    print(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67fd84-40f5-4d84-8e0c-4e5edd52d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training\")\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for images, _, _ in dataloader:\n",
    "        images = images[0].to(device)  # images is a list containing only one view\n",
    "        predictions, targets = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
